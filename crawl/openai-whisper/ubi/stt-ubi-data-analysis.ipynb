{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df97ef59",
   "metadata": {},
   "source": [
    "# STT Evaluation Metrics Visualization Notebook\n",
    "\n",
    "| **Questions**| **Answers**|\n",
    "|---------------------------------------------------|-|\n",
    "| What is the difference between Ubuntu and UBI and is it worth switching? | |\n",
    "| How big are the decompressed images with embedded models? | |\n",
    "| What is the security posture (CVE report) of these model images (packages vs. model)? | |\n",
    "| What is the role of ground truth and overall model evaluation (simple vs. complex data)? | |\n",
    "| Performance gains from cold vs. warm start performance.  | |\n",
    "| What can the model autodetect (FP, Language, Task)? | |\n",
    "| What is the performance gains of basic inference flags?   | |\n",
    "| The impact of advanced decoding arguments on speed and quality. | |\n",
    "| Do advanced arguments / hyperparameters improve accuracy? | |\n",
    "| How to measure accuracy for Audio models (Experiment vs. Production)?  | |\n",
    "| Container placement on CPU and GPU.                | |\n",
    "| The importance of scheduling batch jobs, parallel processing and saturation. | |\n",
    "| Making data driven decisions from experiments.     | |\n",
    "| What combinations of model-size, processor, argument flags, and start type performed the best on different audio samples.       | |\n",
    "| Should you target CPU, GPU or Both?                | |\n",
    "| Should you always have models warm?                | |\n",
    "| What was the fastest transcription?                | |\n",
    "| What was the slowest transcription?                | |\n",
    "| What was the most accurate on complex audio files? | |\n",
    "| What are useful metrics?                           | |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d3f6d5",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08146459",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set Seaborn style\n",
    "sns.set(style=\"whitegrid\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4819445",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc22ea8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "aiml_df = pd.read_csv('data/metrics/g6-12xlarge/ubuntu/aiml_functional_metrics.csv')\n",
    "system_df = pd.read_csv('data/metrics/g6-12xlarge/ubuntu/system_non_functional_metrics.csv')\n",
    "#image_df = pd.read_csv('image_sizes.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4032891e",
   "metadata": {},
   "source": [
    "## Explore Functional Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dadaad",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Display the first few rows\n",
    "aiml_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811a8785",
   "metadata": {},
   "source": [
    "## Visualize Functional Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b16959",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Plot tokens per second by model\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=aiml_df, x='model', y='tokens_per_second', hue='mode')\n",
    "plt.title('Tokens per Second by Model and Mode')\n",
    "plt.ylabel('Tokens per Second')\n",
    "plt.xlabel('Model')\n",
    "plt.legend(title='Mode')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f79e8ce",
   "metadata": {},
   "source": [
    "What is the difference between Ubuntu and UBI and is it worth switching?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083bb771",
   "metadata": {},
   "source": [
    "How many experiments were run, with how many different models on how many different audio samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e82867",
   "metadata": {},
   "source": [
    "How big are the decompressed images with embedded models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18d7b84",
   "metadata": {},
   "source": [
    "What is the role of ground truth and overall model evaluation (simple vs. complex data)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d15456",
   "metadata": {},
   "source": [
    "Performance gains from cold vs. warm start performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60248306",
   "metadata": {},
   "source": [
    "What can the model autodetect (FP, Language, Task)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed4b37a",
   "metadata": {},
   "source": [
    "What is the performance gains of basic inference flags?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442b7fd6",
   "metadata": {},
   "source": [
    "The impact of advanced decoding arguments on speed and quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a4287f",
   "metadata": {},
   "source": [
    "Do advanced arguments / hyperparameters improve accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559246a0",
   "metadata": {},
   "source": [
    "How to measure accuracy for Audio models (Experiment vs. Production)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45aff408",
   "metadata": {},
   "source": [
    "What combinations of model-size, processor, argument flags, and start type performed the best on different audio samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5730a6",
   "metadata": {},
   "source": [
    "Should you target CPU, GPU or Both?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3602b0c4",
   "metadata": {},
   "source": [
    "Should you always have models warm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fef440e",
   "metadata": {},
   "source": [
    "What was the fastest transcription?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539346c4",
   "metadata": {},
   "source": [
    "What was the slowest transcription?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c40868",
   "metadata": {},
   "source": [
    "What was the most accurate on complex audio files?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
